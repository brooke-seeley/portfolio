[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! This site shows my journey learning data science and analytics. Here you’ll find projects that demonstrate what I’ve learned and discovered.\n\n\nThis portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages.\n\n\n\n\nProgramming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data\n\n\n\n\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Learn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Image of Brooke Seeley"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\nI am a full-time student at Brigham Young University with a 3.84 GPA. I am a Heritage Scholarship recipient at BYU and a 4-year Hispanic Scholarship Fund Scholar. I graduate in April 2026 and will be beginning work in May as a Technical Solutions Engineer at Epic.\nI have completed various courses in Mathematics and Statistics as I work toward my degree in Applied Statistics & Analytics. I also took a course in Computer Science (learning Python) and am currently completing multiple online industry courses and certifications (SQL, more Python, Tableau, AWS, Microsoft Azure).\nI was the Brigham Young University Statistics Association Secretary for the 2024-2025 school year. I currently serve as a Teaching Assistant for the BYU Statistics Department and as a Grader for the BYU Mathematics Department. I also volunteer with BYU Football in the Recruiting Department. In my classes and as a volunteer, I have completed various projects related to data science and analysis, which can be viewed on this portfolio or on my GitHub (link below).\nThank you for your interest! Please contact me if you would like more information!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nBachelor’s In Statistics - Applied Statistics and Analytics - Brigham Young University, April 2026\nRelevant Coursework:\n\nAnalysis of Correlated Data\nApplications in Biostatistics\nApplied R Programming\nCalculus I & II\nElementary & Computational Linear Algebra\nData Science Ecosystems & Process\nHow to Program (Python)\nIndustry Micro-Certifications in Data Science and AI\nMethods of Survey Sampling\nPredictive Analytics\nPrinciples of Statistics\nProbability and Inference I & II\nStatistical Modeling I & II\nTechnical Communication"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python, R, SQL, Linux\nData Analysis: Pandas, NumPy, Predictive Modeling & Analytics, Linear Regression, ANOVA\nVisualization: Matplotlib, Seaborn, ggplot, Tableau\nMachine Learning: Scikit-learn, AWS, Microsoft Azure\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\nHealthcare\nSports Analytics\nPredictive Modeling"
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\nDescribe what you hope to achieve through your data science journey:\n\nShort-term learning objectives\nLong-term career aspirations\nTypes of problems you want to solve"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: brookeseeleyaz@gmail.com\nGitHub: github.com/brooke-seeley\nLinkedIn: linkedin.com/in/brooke-seeley\nKaggle: kaggle.com/brookeseeley\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nTechnical Solutions Engineer - Epic - Beginning May 2026\nRecruiting Volunteer - BYU Football - Aug 2025-Present\nTeaching Assistant - BYU Statistics Department - Aug 2024-Present\nLinear Algebra Grader - BYU Mathematics Department - Dec 2024-Present\nExecutive Secretary - BYU Statistics Association - Apr 2024-Apr 2025"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About Me",
    "section": "Hobbies",
    "text": "Hobbies\n\nMusic\nDancing\nGaming\nYouTube"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "",
    "text": "Last semester, I completed a Predictive Analytics course, which had a focus on machine learning practices in R. In this course, we practiced our skills in multiple Kaggle competitions. My final project of the course was with the Kobe Bryant Shot Selection data, where we are given many factors and are asked to predict whether or not Kobe made an attempted shot. Before I was able to predict on this data, I had to do quite a bit of feature engineering. I also had to try various models before reaching a low log-loss. I decided to frame my work as a data cleaning and predition tutorial in this blog. In it, you will find the following:\n\nInitial Preparation\n\nNecessary Packages\nRead in Data\nSplit into Train and Test Data\n\nFirst Recipe\n\nNew Features\nFactor Conversion\nRemoved Features\n\nPreliminary Models\n\nPenalized Logistic Regression\nRandom Forest\nK-Nearest Neighbors\n\nNew Recipe\n\nCreate New Features\nRemove Other Features\n\nFinal Model\n\nThese steps and models can be applied to many different datasets. Feel free to try this with the Kobe dataset, or your own! How low can you get the log-loss?"
  },
  {
    "objectID": "tutorial.html#my-predictive-experience",
    "href": "tutorial.html#my-predictive-experience",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "",
    "text": "Last semester, I completed a Predictive Analytics course, which had a focus on machine learning practices in R. In this course, we practiced our skills in multiple Kaggle competitions. My final project of the course was with the Kobe Bryant Shot Selection data, where we are given many factors and are asked to predict whether or not Kobe made an attempted shot. Before I was able to predict on this data, I had to do quite a bit of feature engineering. I also had to try various models before reaching a low log-loss. I decided to frame my work as a data cleaning and predition tutorial in this blog. In it, you will find the following:\n\nInitial Preparation\n\nNecessary Packages\nRead in Data\nSplit into Train and Test Data\n\nFirst Recipe\n\nNew Features\nFactor Conversion\nRemoved Features\n\nPreliminary Models\n\nPenalized Logistic Regression\nRandom Forest\nK-Nearest Neighbors\n\nNew Recipe\n\nCreate New Features\nRemove Other Features\n\nFinal Model\n\nThese steps and models can be applied to many different datasets. Feel free to try this with the Kobe dataset, or your own! How low can you get the log-loss?"
  },
  {
    "objectID": "tutorial.html#step-1-initial-preparation",
    "href": "tutorial.html#step-1-initial-preparation",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "Step 1: Initial Preparation",
    "text": "Step 1: Initial Preparation\n\nNecessary Packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(vroom)\nlibrary(embed)\nThese 4 packages are key for machine learning and predictive analytics in R.\n-tidyverse: the ultimate R package, especially for tidying and manipulating -tidymodels: makes it easier to create multiple models by using recipes -vroom: quickly reads in and writes datasets, especially when in .csv format -embed: encodes categorical variables in data recipes\n\n\nRead in Data\ndata &lt;- vroom('data.csv')\nvroom() reads in this large dataset super easily, as opposed to a function like read.csv() which can be more computationally expensive.\n\n\nSplit into Train and Test Data\nAs opposed to many other Kaggle competitions, the train and test data came in one .csv file. So, we will need to split it ourselves. When looking at the data, there is a shot_made_flag variable, which is what we are predicting. It has a 0 if the shot wasn’t made, and a 1 if it was. Some of the rows have NA values, telling us that these are the ones we are supposed to predict.\nWe can use the R functions drop_na() to give us the train data, and is.na() for the test data:\ntrainData &lt;- data %&gt;%\n  drop_na(shot_made_flag) %&gt;%\n  mutate(shot_made_flag = factor(shot_made_flag))\n\ntestData &lt;- data %&gt;%\n  filter(is.na(shot_made_flag)) %&gt;%\n  select(-shot_made_flag)\nMy goal for the assignment was to get a log-loss (the score for this Kaggle competition) close to or lower than 0.6. Let’s see if we can do that!"
  },
  {
    "objectID": "tutorial.html#step-2-first-recipe",
    "href": "tutorial.html#step-2-first-recipe",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "Step 2: First Recipe",
    "text": "Step 2: First Recipe\nThis tidymodels recipe was my first draft. Let’s discuss each line of it. Then, think of what you would add or remove to improve it. Click on the link above to the Kaggle competition to get a preview of the data, specifically its variables.\ntarget_recipe &lt;- recipe(shot_made_flag ~ ., data = trainData) %&gt;%\n  step_date(game_date, features=\"month\") %&gt;%\n  step_date(game_date, features=\"year\") %&gt;%\n  step_mutate(game_event_id = factor(game_event_id)) %&gt;%\n  step_mutate(period = factor(period)) %&gt;%\n  step_mutate(playoffs = factor(playoffs)) %&gt;%\n  step_mutate(game_date_month = factor(game_date_month)) %&gt;%\n  step_mutate(game_date_year = factor(game_date_year)) %&gt;%\n  step_rm(shot_id, team_name, team_id, matchup, game_id, game_date) %&gt;%\n  step_other(all_nominal_predictors(), threshold = 0.001) %&gt;%\n  step_lencode_mixed(all_factor_predictors(),\n                     outcome = vars(shot_made_flag)) %&gt;%\n  step_normalize(all_factor_predictors())\n\nAs discussed above, shot_made_flag is our response variable and what we are predicting based on various factors.\nThere was a game_date variable, which a lot could be done with. I added a month feature to see if the time of year during the season affected Kobe’s accuracy.\nI also added a year feature to observe his improvement over his career.\ngame_event_id gave a number for what number shot it was in the game. I made this a factor, would you have done the same?\nThe NBA has 4 quarters, and shot accuracy may change based on the quarter. The dataset had the period listed as simply 1-4, so I converted that to a factor.\nIf it was the playoffs, the value for playoffs was 1, and 0 if not. I made that a factor as well.\nI made the above game_date_month variable I created into a factor.\nI did the same for game_date_year.\nI removed the following variables for these reasons:\n\nshot_id - just identified which shot it was, not helpful for predicting\nteam_name and team_id - this was for his own team, and Kobe was a Lakers lifer\nmatchup and game_id - these became redundant thanks to other variables, such as opponent\ngame_date - we already created game_date_month and game_date_year which explain more\n\nWith target encoding, I wanted to combine the variables of very small amounts so it didn’t become an overfit model.\nThis line is code for target encoding of variables.\nI normalized the factor predictors to help with the fit."
  },
  {
    "objectID": "tutorial.html#step-3-preliminary-models",
    "href": "tutorial.html#step-3-preliminary-models",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "Step 3: Preliminary Models",
    "text": "Step 3: Preliminary Models\n\nModel 1: Penalized Logistic Regression\nI first started with a simpler model, Penalized Logistic Regression. I used a workflow function to make the modeling easier, and ran a cross-validation to tune the best values for mixture and penalty. I used brier_class as the metric, since that is the definer for log-loss. Note that to use this model, you will also need the glmnet library.\nHere is the code:\nlibrary(glmnet)\n\npreg_mod &lt;- logistic_reg(mixture=tune(), penalty=tune()) %&gt;%\n  set_engine(\"glmnet\")\n\npreg_workflow &lt;- workflow() %&gt;%\n  add_recipe(target_recipe) %&gt;%\n  add_model(preg_mod)\n\n### Grid of values to tune over\n\ntuning_grid &lt;- grid_regular(penalty(),\n                            mixture(),\n                            levels = 5)\n\n### Split data for CV\n\nfolds &lt;- vfold_cv(trainData, v = 5, repeats = 1)\n\n### Run the CV\n\nCV_results &lt;- preg_workflow %&gt;%\n  tune_grid(resamples=folds,\n            grid=tuning_grid,\n            metrics(metric_set(brier_class)))\n\n### Find Best Tuning Parameters\n\nbestTune &lt;- CV_results %&gt;%\n  select_best(metric=\"brier_class\")\n\n### Finalize the Workflow & fit it\n\nfinal_wf &lt;-\n  preg_workflow %&gt;%\n  finalize_workflow(bestTune) %&gt;%\n  fit(data=trainData)\n\n### Predict\n\npen_reg_predictions &lt;- final_wf %&gt;%\n  predict(new_data = testData, type=\"prob\")\n\n### Kaggle\n\npen_reg_kaggle_submission &lt;- pen_reg_predictions %&gt;%\n  bind_cols(., testData) %&gt;%\n  select(shot_id, .pred_1) %&gt;%\n  rename(shot_made_flag=.pred_1)\n\nvroom_write(x=pen_reg_kaggle_submission, file=\"./PenRegPreds.csv\", delim=',')\nNote the “Kaggle” section of the code. This was to get the predictions into the proper format for Kaggle scoring. You need to do something similar for every Kaggle competition. That is what the sample submission document is for, to give you an example of what your submission should look like! I recommend naming the files very specifically to make it easier to see what model gave you what Kaggle score.\nThis gave me a log-loss of 0.61244, which is quite good! Perhaps with your seed, it could be lower or higher. Let’s see if we can take that down a notch.\n\n\nModel 2: Random Forest\nNext is a popular model for binary or categorical predictions, which is a Random Forest model. This creates a bunch of regression trees for each row and averages their results. You’ll see that this workflow looks very similar to the one for the last model, and is a standard ML/Predictive Analytics workflow.\nNote that I set the trees myself. You can tune for them, but it becomes very computationally expensive. This time, we are tuning for mtry, the number of features considered at each node split, and min_n, the number of datapoints in a node for it to split. With the rand_forest function, I set the mode as \"classification\" so it will give us a 1 or 0 for the shot rather than a probability.\nlibrary(rpart)\n\ntree_mod &lt;- rand_forest(mtry=tune(),\n                        min_n=tune(),\n                        trees=500) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\ntree_workflow &lt;- workflow() %&gt;%\n  add_recipe(target_recipe) %&gt;%\n  add_model(tree_mod)\n\n## Grid of values to tune over\n\ntuning_grid &lt;- grid_regular(mtry(range=c(1,20)),\n                            min_n(),\n                            levels=5)\n\n### CV\n\nfolds &lt;- vfold_cv(trainData, v = 5, repeats = 1)\n\nCV_results &lt;- tree_workflow %&gt;%\n  tune_grid(resamples=folds,\n            grid=tuning_grid,\n            metrics=(metric_set(brier_class)))\n\n### Find best tuning parameters\n\nbestTune &lt;- CV_results %&gt;%\n  select_best(metric=\"brier_class\")\n\n### Finalize workflow\n\nfinal_wf &lt;-\n  tree_workflow %&gt;%\n  finalize_workflow(bestTune) %&gt;%\n  fit(data=trainData)\n\n### Predict\n\ntree_predictions &lt;- final_wf %&gt;%\n  predict(new_data = testData, type=\"prob\")\n\n### Kaggle\n\ntree_kaggle_submission &lt;- tree_predictions %&gt;%\n  bind_cols(., testData) %&gt;%\n  select(shot_id, .pred_1) %&gt;%\n  rename(shot_made_flag=.pred_1)\n\nvroom_write(x=tree_kaggle_submission, file=\"./RegTreePreds.csv\", delim=',')\nHere, I got a log-loss of 0.61162, which is a slight improvement on Penalized Regression. How will our last model do?\n\n\nModel 3: K-Nearest Neighbors\nFinally, I tried K-Nearest Neighbors, which is specifically a ML algorithm. It makes predictions based on a majority vote of the training data points around our test points. The number of points it looks at is decided by the neighbors argument, which is what we will tune.\nlibrary(kknn)\n\nknn_model &lt;- nearest_neighbor(neighbors=tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kknn\")\n\nknn_workflow &lt;- workflow() %&gt;%\n  add_recipe(target_recipe) %&gt;%\n  add_model(knn_model)\n\n### Tuning Parameters\n\ntuning_grid &lt;- grid_regular(neighbors())\n\n### CV\n\nfolds &lt;- vfold_cv(trainData, v = 5, repeats = 1)\n\nCV_results &lt;- knn_workflow %&gt;%\n  tune_grid(resamples=folds,\n            grid=tuning_grid,\n            metrics(metric_set(brier_class)))\n\n### Find best K\n\nbestTune &lt;- CV_results %&gt;%\n  select_best(metric=\"brier_class\")\n\n### Finalize Workflow\n\nfinal_wf &lt;-\n  knn_workflow %&gt;%\n  finalize_workflow(bestTune) %&gt;%\n  fit(data=trainData)\n\n### Predict\n\nknn_predictions &lt;- final_wf %&gt;%\n  predict(knn_workflow, new_data=testData, type=\"prob\")\n\n### Kaggle\n\nknn_kaggle_submission &lt;- knn_predictions %&gt;%\n  bind_cols(., testData) %&gt;%\n  select(shot_id, .pred_1) %&gt;%\n  rename(shot_made_flag=.pred_1)\n\nvroom_write(x=knn_kaggle_submission, file=\"./KNNTreePreds.csv\", delim=',')\nOur log loss went up to 0.97432, which is a major downgrade. So, clearly we should stick to one of the above. Maybe we could adjust our recipe a bit too?"
  },
  {
    "objectID": "tutorial.html#step-4-new-recipe",
    "href": "tutorial.html#step-4-new-recipe",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "Step 4: New Recipe",
    "text": "Step 4: New Recipe\nnew_recipe &lt;- recipe(shot_made_flag ~ ., data = trainData) %&gt;%\n  step_date(game_date, features=\"month\") %&gt;%\n  step_date(game_date, features=\"year\") %&gt;%\n  step_mutate(home_away = if_else(str_detect(matchup, \"@\"), \"away\", \"home\")) %&gt;%\n  step_mutate(game_event_id = factor(game_event_id)) %&gt;%\n  step_mutate(period = factor(period)) %&gt;%\n  step_mutate(playoffs = factor(playoffs)) %&gt;%\n  step_mutate(home_away = factor(home_away)) %&gt;%\n  step_mutate(game_date_month = factor(game_date_month)) %&gt;%\n  step_mutate(game_date_year = factor(game_date_year)) %&gt;%\n  step_rm(shot_id, team_name, team_id, matchup, game_id, game_date,\n          combined_shot_type) %&gt;%\n  step_other(all_nominal_predictors(), threshold = 0.001) %&gt;%\n  step_lencode_mixed(all_factor_predictors(),\n                     outcome = vars(shot_made_flag)) %&gt;%\n  step_normalize(all_factor_predictors())\nHere, I made some adjustments to the recipe.\n\nI made a new feature called home_away, where I looked in the matchup variable to see whether the game was home or away. I did this by detecting @ signs, which would tell us that it was an away game. I made that a factor as well.\nI removed another variable, combined_shot_type, since it was very similar to action_type but had less information.\n\nAnything else you would change?"
  },
  {
    "objectID": "tutorial.html#step-5-final-model",
    "href": "tutorial.html#step-5-final-model",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "Step 5: Final Model",
    "text": "Step 5: Final Model\nSeeing as the Random Forest did the best before, perhaps we could tune it a bit more. I did a very similar workflow, but changed my tuning grid. This one gave me the best results, in which I shortened the mtry range and added one for min_n.\ntuning_grid &lt;- grid_regular(mtry(range = c(2, 8)),\n                            min_n(range = c(60, 120)),\n                            levels=5)\nThe best mtry ended up being 4, and the best min_n was 120. What did it look like for you?"
  },
  {
    "objectID": "tutorial.html#conclusion",
    "href": "tutorial.html#conclusion",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "Conclusion",
    "text": "Conclusion\nThe above model gave me a final Kaggle score of 0.60771, which is super close to our goal!\nI did try other models to get lower, such as tuning trees and and XGBoost model, but they were either much too computationally expensive or did not lessen my log-loss.\nWhat models would you try? Would you change the recipe at all? I encourage you to try this competition out for yourself, it was a lot of fun! You should also try this workflow on a different Kaggle competition, and adjust it to what works for you.\nGood luck predicting!"
  },
  {
    "objectID": "tutorial.html#more-from-me",
    "href": "tutorial.html#more-from-me",
    "title": "Tutorial Blog - Cleaning and Predicting from Data in R",
    "section": "More From Me",
    "text": "More From Me\n\nA Kaggle Notebook describing this project: Kobe Bryant Notebook\nMy GitHub for this project: Kobe Bryant Repository"
  }
]